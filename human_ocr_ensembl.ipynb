{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0f8e486",
   "metadata": {
    "papermill": {
     "duration": 0.008009,
     "end_time": "2022-06-03T07:01:20.856563",
     "exception": false,
     "start_time": "2022-06-03T07:01:20.848554",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### this is the sample notebook taken directly from the repo's github.\n",
    "\n",
    "# TF CNN Classifier\n",
    "\n",
    "To run this notebook on an another benchmark, use\n",
    "\n",
    "```\n",
    "papermill utils/tf_cnn_classifier.ipynb tf_cnn_experiments/[DATASET NAME].ipynb -p DATASET [DATASET NAME]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "29d4e18d-144a-4117-8726-53786420dbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/justi/Desktop/Home/Hackathon/HackOS_1/genomic\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "afbb9431",
   "metadata": {
    "papermill": {
     "duration": 0.011519,
     "end_time": "2022-06-03T07:01:20.872885",
     "exception": false,
     "start_time": "2022-06-03T07:01:20.861366",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# DATASET = 'demo_coding_vs_intergenomic_seqs'\n",
    "VERSION = 0\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74dfcfa2",
   "metadata": {
    "papermill": {
     "duration": 0.007177,
     "end_time": "2022-06-03T07:01:20.883787",
     "exception": false,
     "start_time": "2022-06-03T07:01:20.876610",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "DATASET = \"human_ocr_ensembl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "982d14ab",
   "metadata": {
    "papermill": {
     "duration": 0.011862,
     "end_time": "2022-06-03T07:01:20.902179",
     "exception": false,
     "start_time": "2022-06-03T07:01:20.890317",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human_ocr_ensembl 0 64 10\n"
     ]
    }
   ],
   "source": [
    "print(DATASET, VERSION, BATCH_SIZE, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42abd76e-98e1-4d97-9419-8f98f1745971",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a06a600",
   "metadata": {
    "papermill": {
     "duration": 0.004126,
     "end_time": "2022-06-03T07:01:20.910497",
     "exception": false,
     "start_time": "2022-06-03T07:01:20.906371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1a57e0d2",
   "metadata": {
    "papermill": {
     "duration": 16.400311,
     "end_time": "2022-06-03T07:01:37.317938",
     "exception": false,
     "start_time": "2022-06-03T07:01:20.917627",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "import numpy as np\n",
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "from genomic_benchmarks.data_check import is_downloaded, info\n",
    "from genomic_benchmarks.models.tf import vectorize_layer\n",
    "from genomic_benchmarks.models.tf import get_basic_cnn_model_v0 as get_model\n",
    "\n",
    "# if not is_downloaded(DATASET):\n",
    "#     download_dataset(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2519f4f-ce25-4a06-bd15-b89984c9aed0",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Constructor parameter should be str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgenomic_benchmarks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mloc2seq\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m download_dataset\n\u001b[0;32m----> 3\u001b[0m \u001b[43mdownload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhuman_ocr_ensembl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/justi/Desktop/Home/Hackathon/HackOS_1/dev3.8/lib/python3.8/site-packages/genomic_benchmarks/loc2seq/loc2seq.py:48\u001b[0m, in \u001b[0;36mdownload_dataset\u001b[0;34m(interval_list_dataset, version, dest_path, cache_path, force_download, use_cloud_cache, local_repo)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03mTransform an interval-list genomic dataset into a full-seq genomic dataset.\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03m                seq_dataset_path (Path): Path to the full-seq dataset.\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m interval_list_dataset \u001b[38;5;241m=\u001b[39m _guess_location(interval_list_dataset, local_repo)\n\u001b[0;32m---> 48\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_check_dataset_existence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval_list_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_repo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m _get_dataset_name(interval_list_dataset)\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/justi/Desktop/Home/Hackathon/HackOS_1/dev3.8/lib/python3.8/site-packages/genomic_benchmarks/utils/datasets.py:42\u001b[0m, in \u001b[0;36m_check_dataset_existence\u001b[0;34m(interval_list_dataset, version, local_repo)\u001b[0m\n\u001b[1;32m     40\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(fr)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[43mURL\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval_list_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;28mstr\u001b[39m(url \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m fr:\n\u001b[1;32m     44\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(fr)\n",
      "File \u001b[0;32m/mnt/c/Users/justi/Desktop/Home/Hackathon/HackOS_1/dev3.8/lib/python3.8/site-packages/yarl/_url.py:269\u001b[0m, in \u001b[0;36mURL.__new__\u001b[0;34m(cls, val, encoded, strict)\u001b[0m\n\u001b[1;32m    267\u001b[0m     val \u001b[38;5;241m=\u001b[39m urlsplit(\u001b[38;5;28mstr\u001b[39m(val))\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConstructor parameter should be str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    271\u001b[0m cache: _InternalURLCache \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encoded:\n",
      "\u001b[0;31mTypeError\u001b[0m: Constructor parameter should be str"
     ]
    }
   ],
   "source": [
    "from genomic_benchmarks.loc2seq import download_dataset\n",
    "\n",
    "download_dataset(\"human_ocr_ensembl\", version=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ef313c21",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Constructor parameter should be str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATASET\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/c/Users/justi/Desktop/Home/Hackathon/HackOS_1/dev3.8/lib/python3.8/site-packages/genomic_benchmarks/data_check/info.py:28\u001b[0m, in \u001b[0;36minfo\u001b[0;34m(interval_list_dataset, version, local_repo)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03mPrint info about the bechmark.\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m                DataFrame with counts of seqeunces for each class in a training and testing sets.\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     27\u001b[0m interval_list_dataset \u001b[38;5;241m=\u001b[39m _guess_location(interval_list_dataset, local_repo)\n\u001b[0;32m---> 28\u001b[0m metadata \u001b[38;5;241m=\u001b[39m \u001b[43m_check_dataset_existence\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval_list_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_repo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m dataset_name \u001b[38;5;241m=\u001b[39m _get_dataset_name(interval_list_dataset)\n\u001b[1;32m     31\u001b[0m dfs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m/mnt/c/Users/justi/Desktop/Home/Hackathon/HackOS_1/dev3.8/lib/python3.8/site-packages/genomic_benchmarks/utils/datasets.py:42\u001b[0m, in \u001b[0;36m_check_dataset_existence\u001b[0;34m(interval_list_dataset, version, local_repo)\u001b[0m\n\u001b[1;32m     40\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(fr)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[43mURL\u001b[49m\u001b[43m(\u001b[49m\u001b[43minterval_list_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39murlopen(\u001b[38;5;28mstr\u001b[39m(url \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mas\u001b[39;00m fr:\n\u001b[1;32m     44\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(fr)\n",
      "File \u001b[0;32m/mnt/c/Users/justi/Desktop/Home/Hackathon/HackOS_1/dev3.8/lib/python3.8/site-packages/yarl/_url.py:269\u001b[0m, in \u001b[0;36mURL.__new__\u001b[0;34m(cls, val, encoded, strict)\u001b[0m\n\u001b[1;32m    267\u001b[0m     val \u001b[38;5;241m=\u001b[39m urlsplit(\u001b[38;5;28mstr\u001b[39m(val))\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConstructor parameter should be str\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    271\u001b[0m cache: _InternalURLCache \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m encoded:\n",
      "\u001b[0;31mTypeError\u001b[0m: Constructor parameter should be str"
     ]
    }
   ],
   "source": [
    "info(DATASET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ce24e0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TF Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "409118c7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 139804 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#SEQ_PATH = Path.home() / '.genomic_benchmarks' / DATASET\n",
    "SEQ_PATH = Path(DATASET)\n",
    "CLASSES = [x.stem for x in (SEQ_PATH/'train').iterdir() if x.is_dir()]\n",
    "NUM_CLASSES = len(CLASSES)\n",
    "\n",
    "train_dset = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    SEQ_PATH / 'train',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_names=CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fa20a786",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_BatchDataset element_spec=(TensorSpec(shape=(None,), dtype=tf.string, name=None), TensorSpec(shape=(None,), dtype=tf.int32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "if NUM_CLASSES > 2:\n",
    "    train_dset = train_dset.map(lambda x, y: (x, tf.one_hot(y, depth=NUM_CLASSES)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9706c8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Text vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "60ac5547",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 't', 'a', 'g', 'c', 'n']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer.adapt(train_dset.map(lambda x, y: x))\n",
    "VOCAB_SIZE = len(vectorize_layer.get_vocabulary())\n",
    "vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c8a30",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vectorize_text(text, label):\n",
    "  text = tf.expand_dims(text, -1)\n",
    "  return vectorize_layer(text)-2, label\n",
    "\n",
    "print(text)\n",
    "\n",
    "train_ds = train_dset.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216d2dd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fbf777e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = get_model(NUM_CLASSES, VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae2ebd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2185/2185 [==============================] - 95s 42ms/step - loss: 0.6311 - binary_accuracy: 0.6417 - f1_score: 0.4172\n",
      "Epoch 2/10\n",
      "2185/2185 [==============================] - 95s 43ms/step - loss: 0.5930 - binary_accuracy: 0.6822 - f1_score: 0.5713\n",
      "Epoch 3/10\n",
      "2185/2185 [==============================] - 96s 44ms/step - loss: 0.5762 - binary_accuracy: 0.6985 - f1_score: 0.6074\n",
      "Epoch 4/10\n",
      "2185/2185 [==============================] - 99s 45ms/step - loss: 0.5665 - binary_accuracy: 0.7045 - f1_score: 0.6235\n",
      "Epoch 5/10\n",
      "2185/2185 [==============================] - 98s 45ms/step - loss: 0.5603 - binary_accuracy: 0.7105 - f1_score: 0.6324\n",
      "Epoch 6/10\n",
      "2185/2185 [==============================] - 100s 46ms/step - loss: 0.5560 - binary_accuracy: 0.7145 - f1_score: 0.6397\n",
      "Epoch 7/10\n",
      "2185/2185 [==============================] - 101s 46ms/step - loss: 0.5522 - binary_accuracy: 0.7163 - f1_score: 0.6432\n",
      "Epoch 8/10\n",
      "2185/2185 [==============================] - 104s 47ms/step - loss: 0.5488 - binary_accuracy: 0.7204 - f1_score: 0.6487\n",
      "Epoch 9/10\n",
      "1584/2185 [====================>.........] - ETA: 26s - loss: 0.5448 - binary_accuracy: 0.7237 - f1_score: 0.6540"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bae33fa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Evaluation on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e009a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dset = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    SEQ_PATH / 'test',\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_names=CLASSES)\n",
    "\n",
    "if NUM_CLASSES > 2:\n",
    "    test_dset = test_dset.map(lambda x, y: (x, tf.one_hot(y, depth=NUM_CLASSES)))\n",
    "test_ds =  test_dset.map(vectorize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4543bcf7",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 17.864826,
   "end_time": "2022-06-03T07:01:37.782465",
   "environment_variables": {},
   "exception": null,
   "input_path": "utils/tf_cnn_classifier.ipynb",
   "output_path": "tf_cnn_experiments/human_ocr_ensembl.ipynb",
   "parameters": {
    "DATASET": "human_ocr_ensembl"
   },
   "start_time": "2022-06-03T07:01:19.917639",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
